{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research API: Query Refinement\n",
    "\n",
    "Refine vague research queries through interactive clarification before executing research.\n",
    "\n",
    "**What you'll learn:**\n",
    "\n",
    "- Use an LLM to identify underspecified queries\n",
    "- Build an interactive clarification loop\n",
    "- Generate refined, detailed research prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U tavily-python langchain-openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import time\n",
    "\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY:\\n\")\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "client = TavilyClient()\n",
    "llm = ChatOpenAI(model=\"gpt-5.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Clarification Logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClarificationResponse(BaseModel):\n",
    "    \"\"\"Structured response for query clarification.\"\"\"\n",
    "    needs_clarification: bool = Field(description=\"True if more info needed\")\n",
    "    message: str = Field(description=\"Follow-up questions OR refined query\")\n",
    "\n",
    "PROMPT = \"\"\"You are a research assistant refining a query through conversation.\n",
    "\n",
    "Original topic: {query}\n",
    "Conversation: {conversation}\n",
    "\n",
    "If you need more details, set needs_clarification=True and ask 2-3 questions.\n",
    "If you have enough context, set needs_clarification=False and provide the refined query.\n",
    "\"\"\"\n",
    "\n",
    "def clarify(query: str, conversation: list, force_final: bool = False) -> ClarificationResponse:\n",
    "    \"\"\"Get clarification or refined query from LLM.\"\"\"\n",
    "    conv_text = \"\\n\".join(f\"{m['role'].title()}: {m['content']}\" for m in conversation) or \"(none)\"\n",
    "    prompt = PROMPT.format(query=query, conversation=conv_text)\n",
    "    if force_final:\n",
    "        prompt += \"\\nProvide the best possible refined query now.\"\n",
    "    return llm.with_structured_output(ClarificationResponse).invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Query Refinement\n",
    "\n",
    "> **Note:** Uses `input()` for interactive prompts. Replace with hardcoded strings if your environment doesn't support stdin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 3\n",
    "\n",
    "initial_query = input(\"What would you like to research?\\n> \")\n",
    "conversation = []\n",
    "\n",
    "for i in range(max_iterations):\n",
    "    response = clarify(initial_query, conversation)\n",
    "    \n",
    "    if not response.needs_clarification:\n",
    "        refined_query = response.message\n",
    "        print(f\"\\nâœ… Refined query:\\n{refined_query}\")\n",
    "        break\n",
    "    \n",
    "    print(f\"\\nðŸ¤– Assistant:\\n{response.message}\")\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": response.message})\n",
    "    \n",
    "    user_input = input(\"\\n> \")\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "else:\n",
    "    response = clarify(initial_query, conversation, force_final=True)\n",
    "    refined_query = response.message\n",
    "    print(f\"\\nâœ… Refined query:\\n{refined_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Research\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.research(input=refined_query, model=\"mini\")\n",
    "request_id = result[\"request_id\"]\n",
    "\n",
    "response = client.get_research(request_id)\n",
    "\n",
    "while response[\"status\"] not in [\"completed\", \"failed\"]:\n",
    "    print(f\"Status: {response['status']}... polling again in 10 seconds\")\n",
    "    time.sleep(10)\n",
    "    response = client.get_research(request_id)\n",
    "\n",
    "if response[\"status\"] == \"failed\":\n",
    "    raise RuntimeError(f\"Research failed: {response.get('error', 'Unknown error')}\")\n",
    "\n",
    "print(\"\\nâœ… Research Complete!\")\n",
    "display(Markdown(response[\"content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.get(\"sources\", [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- See [Hybrid Research](./hybrid_research.ipynb) to combine with internal data\n",
    "- See [Structured Output](./structured_output.ipynb) for custom response schemas\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
