{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research API: Hybrid Research\n",
    "\n",
    "Combine Tavily's web research with your internal data for comprehensive coverage.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Generate sub-queries for internal RAG\n",
    "- Create a web research task from internal findings\n",
    "- Synthesize internal + external data into a final report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U tavily-python langchain-openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import time\n",
    "\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY:\\n\")\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "client = TavilyClient()\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Query Internal Data\n",
    "\n",
    "Generate sub-queries and run against your internal RAG system.\n",
    "\n",
    "> **Note:** Replace `internal_rag_search()` with your own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queries(BaseModel):\n",
    "    queries: list[str] = Field(description=\"Sub-queries for internal RAG\")\n",
    "\n",
    "def internal_rag_search(queries: list[str]) -> str:\n",
    "    \"\"\"Replace with your internal RAG implementation.\"\"\"\n",
    "    # Example: query vector DB, data warehouse, or internal docs\n",
    "    return \"Internal RAG results: [placeholder - implement your own]\"\n",
    "\n",
    "task = \"I launched a new AI Meeting Notes feature â€” evaluate customer feedback and competitor products.\"\n",
    "\n",
    "# Generate sub-queries for internal data\n",
    "queries = llm.with_structured_output(Queries).invoke(\n",
    "    f\"Break down this task into 5 queries for internal RAG (usage, feedback, R&D data): {task}\"\n",
    ")\n",
    "\n",
    "internal_results = internal_rag_search(queries.queries)\n",
    "print(f\"Generated queries: {queries.queries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Web Research Task\n",
    "\n",
    "Identify gaps that require external research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_task = llm.invoke(f\"\"\"\n",
    "Given the user's research task and internal findings, generate a web research task.\n",
    "\n",
    "User Task: {task}\n",
    "Internal Findings: {internal_results}\n",
    "\n",
    "Focus on what needs external validation: competitors, reviews, benchmarks, news.\n",
    "Output ONLY the research task.\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Web research task: {web_task.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Tavily Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.research(input=web_task.content, model=\"pro\")\n",
    "request_id = result[\"request_id\"]\n",
    "\n",
    "response = client.get_research(request_id)\n",
    "\n",
    "while response[\"status\"] not in [\"completed\", \"failed\"]:\n",
    "    print(f\"Status: {response['status']}... polling again in 10 seconds\")\n",
    "    time.sleep(10)\n",
    "    response = client.get_research(request_id)\n",
    "\n",
    "if response[\"status\"] == \"failed\":\n",
    "    raise RuntimeError(f\"Research failed: {response.get('error', 'Unknown error')}\")\n",
    "\n",
    "web_report = response[\"content\"]\n",
    "sources = response[\"sources\"]\n",
    "print(\"Web research complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Synthesize Final Report\n",
    "\n",
    "Combine internal and external findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_report = llm.invoke(f\"\"\"\n",
    "Research task: {task}\n",
    "\n",
    "Internal findings:\n",
    "{internal_results}\n",
    "\n",
    "Web research:\n",
    "{web_report}\n",
    "\n",
    "Generate a final report combining both sources. Cite sources appropriately.\n",
    "Output as markdown.\n",
    "\"\"\")\n",
    "\n",
    "print(\"Final report complete!\")\n",
    "display(Markdown(final_report.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- See [Query Refinement](./query_refinement.ipynb) for interactive query refinement\n",
    "- See [Streaming](./streaming.ipynb) for real-time progress updates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
