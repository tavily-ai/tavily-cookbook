{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain + Tavily Integration Cookbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `langchain-tavily` package provides native LangChain tools that integrate seamlessly with LangChain's agent frameworks and LangGraph workflows.\n",
    "\n",
    "By the end of this cookbook, you'll know how to:\n",
    "- Use TavilySearch for real-time web search\n",
    "- Use TavilyExtract to get full page content from URLs\n",
    "- Use TavilyMap to discover site structure and internal links\n",
    "- Use TavilyCrawl for deep website crawling\n",
    "- Use TavilyResearch for comprehensive research tasks\n",
    "- Build a LangGraph agent with all Tavily tools\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "1. **Sign up** for Tavily at [app.tavily.com](https://app.tavily.com/home/) to get your API key.\n",
    "2. **Copy your API key** from your Tavily account dashboard.\n",
    "3. **Run the cells below** to set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-tavily langchain-openai langgraph python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY:\\n\")\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY:\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. TavilySearch - Web Search Tool\n",
    "\n",
    "TavilySearch executes search queries using Tavily's Search API. It returns relevant web results with titles, URLs, content snippets, and relevance scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "# Initialize the search tool with default settings\n",
    "search = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\",\n",
    "    search_depth=\"basic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic search invocation\n",
    "result = search.invoke({\"query\": \"What are the latest developments in AI agents?\"})\n",
    "\n",
    "print(f\"Query: {result['query']}\")\n",
    "print(f\"Response time: {result['response_time']}s\")\n",
    "print(f\"Number of results: {len(result['results'])}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "for i, r in enumerate(result[\"results\"], 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"  Title: {r['title']}\")\n",
    "    print(f\"  URL: {r['url']}\")\n",
    "    print(f\"  Score: {r['score']:.4f}\")\n",
    "    print(f\"  Content: {r['content'][:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News Search with Time Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for recent news\n",
    "news_search = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"news\",\n",
    "    time_range=\"week\",\n",
    ")\n",
    "\n",
    "news_result = news_search.invoke({\"query\": \"OpenAI announcements\"})\n",
    "\n",
    "for i, r in enumerate(news_result[\"results\"], 1):\n",
    "    print(f\"{i}. {r['title']}\")\n",
    "    print(f\"   {r['url']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain-Filtered Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search only specific domains\n",
    "domain_search = TavilySearch(\n",
    "    max_results=5,\n",
    "    include_domains=[\"arxiv.org\", \"openai.com\", \"anthropic.com\"],\n",
    ")\n",
    "\n",
    "domain_result = domain_search.invoke({\"query\": \"large language model research\"})\n",
    "\n",
    "for i, r in enumerate(domain_result[\"results\"], 1):\n",
    "    print(f\"{i}. {r['title']}\")\n",
    "    print(f\"   {r['url']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. TavilyExtract - Content Extraction Tool\n",
    "\n",
    "TavilyExtract retrieves the full content from one or more URLs. It's useful when you need the complete text of a webpage rather than just search snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilyExtract\n",
    "\n",
    "extract = TavilyExtract(\n",
    "    extract_depth=\"basic\",\n",
    "    include_images=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract content from a single URL\n",
    "extract_result = extract.invoke({\n",
    "    \"urls\": [\"https://en.wikipedia.org/wiki/Artificial_intelligence\"]\n",
    "})\n",
    "\n",
    "print(f\"Response time: {extract_result['response_time']}s\")\n",
    "print(f\"Number of results: {len(extract_result['results'])}\")\n",
    "print(f\"Failed results: {len(extract_result['failed_results'])}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "for r in extract_result[\"results\"]:\n",
    "    print(f\"URL: {r['url']}\")\n",
    "    print(f\"Content length: {len(r['raw_content'])} characters\")\n",
    "    print(f\"Preview: {r['raw_content'][:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from multiple URLs at once (up to 20)\n",
    "multi_extract = extract.invoke({\n",
    "    \"urls\": [\n",
    "        \"https://docs.tavily.com\",\n",
    "        \"https://python.langchain.com/docs/introduction/\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "for r in multi_extract[\"results\"]:\n",
    "    print(f\"URL: {r['url']}\")\n",
    "    print(f\"Content length: {len(r['raw_content'])} characters\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. TavilyMap - Site Structure Discovery\n",
    "\n",
    "TavilyMap discovers all internal links from a base URL without extracting content. It's useful for understanding site structure before crawling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilyMap\n",
    "\n",
    "map_tool = TavilyMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map a documentation site\n",
    "map_result = map_tool.invoke({\n",
    "    \"url\": \"www.tavily.com\",\n",
    "    \"instructions\": \"Find all API documentation and integration pages\"\n",
    "})\n",
    "\n",
    "print(f\"Base URL: {map_result['base_url']}\")\n",
    "print(f\"Response time: {map_result['response_time']}s\")\n",
    "print(f\"URLs discovered: {len(map_result['results'])}\")\n",
    "print(\"\\nDiscovered URLs:\")\n",
    "\n",
    "for url in map_result[\"results\"][:15]:\n",
    "    print(f\"  - {url}\")\n",
    "\n",
    "if len(map_result[\"results\"]) > 15:\n",
    "    print(f\"  ... and {len(map_result['results']) - 15} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. TavilyCrawl - Deep Website Crawling\n",
    "\n",
    "TavilyCrawl combines mapping and extraction to crawl entire websites and retrieve content from multiple pages. Use natural language instructions to guide what content to focus on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilyCrawl\n",
    "\n",
    "crawl = TavilyCrawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl documentation for specific content\n",
    "crawl_result = crawl.invoke({\n",
    "    \"url\": \"www.tavily.com\",\n",
    "    \"instructions\": \"Extract API reference and code examples\",\n",
    "})\n",
    "\n",
    "print(f\"Base URL: {crawl_result['base_url']}\")\n",
    "print(f\"Response time: {crawl_result['response_time']}s\")\n",
    "print(f\"Pages crawled: {len(crawl_result['results'])}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "for i, page in enumerate(crawl_result[\"results\"][:3], 1):\n",
    "    print(f\"Page {i}: {page['url']}\")\n",
    "    print(f\"Content preview: {page['raw_content'][:300]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. TavilyResearch - Comprehensive Research\n",
    "\n",
    "TavilyResearch creates comprehensive research reports on complex topics. It automatically searches, synthesizes, and cites sources.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `model` | str | \"auto\" | \"mini\", \"pro\", or \"auto\" |\n",
    "| `citation_format` | str | \"numbered\" | \"numbered\", \"mla\", \"apa\", \"chicago\" |\n",
    "| `stream` | bool | False | Stream results as generated |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilyResearch, TavilyGetResearch\n",
    "import time\n",
    "\n",
    "research = TavilyResearch(\n",
    "    model=\"mini\",\n",
    "    citation_format=\"numbered\",\n",
    ")\n",
    "\n",
    "get_research = TavilyGetResearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a research task\n",
    "research_task = research.invoke({\n",
    "    \"input\": \"What are the key differences between LangChain and LlamaIndex for building RAG applications?\"\n",
    "})\n",
    "\n",
    "print(f\"Research task created:\")\n",
    "print(f\"  Request ID: {research_task['request_id']}\")\n",
    "print(f\"  Status: {research_task['status']}\")\n",
    "print(f\"  Model: {research_task['model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poll for results\n",
    "request_id = research_task[\"request_id\"]\n",
    "\n",
    "while True:\n",
    "    result = get_research.invoke({\"request_id\": request_id})\n",
    "    status = result[\"status\"]\n",
    "    print(f\"Status: {status}\")\n",
    "    \n",
    "    if status == \"completed\":\n",
    "        break\n",
    "    elif status == \"failed\":\n",
    "        print(\"Research failed\")\n",
    "        break\n",
    "    \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "if result[\"status\"] == \"completed\":\n",
    "    display(Markdown(result[\"content\"]))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Sources:\")\n",
    "    for i, source in enumerate(result.get(\"sources\", []), 1):\n",
    "        print(f\"  [{i}] {source['title']}\")\n",
    "        print(f\"      {source['url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Building a LangGraph Agent with Tavily Tools\n",
    "\n",
    "Now let's combine all the Tavily tools into a powerful research agent using LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import datetime\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini-2025-08-07\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Tavily tools for the agent\n",
    "search_tool = TavilySearch(\n",
    "    max_results=10,\n",
    "    search_depth=\"advanced\",\n",
    "    include_raw_content=True,\n",
    ")\n",
    "\n",
    "extract_tool = TavilyExtract(\n",
    "    extract_depth=\"advanced\",\n",
    ")\n",
    "\n",
    "crawl_tool = TavilyCrawl()\n",
    "\n",
    "tools = [search_tool, extract_tool, crawl_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create system prompt\n",
    "today = datetime.datetime.now().strftime(\"%B %d, %Y\")\n",
    "\n",
    "system_prompt = f\"\"\"You are an expert research assistant with access to powerful web tools.\n",
    "\n",
    "Today's date: {today}\n",
    "\n",
    "Your available tools:\n",
    "\n",
    "1. **TavilySearch**: Search the web for information\n",
    "   - Use for finding recent news, articles, and general information\n",
    "   - Returns ranked results with titles, URLs, and content snippets\n",
    "\n",
    "2. **TavilyExtract**: Extract full content from URLs\n",
    "   - Use when you need the complete text of a specific webpage\n",
    "   - Can process up to 20 URLs at once\n",
    "\n",
    "3. **TavilyCrawl**: Crawl websites for deep content\n",
    "   - Use for exploring documentation sites or gathering comprehensive information\n",
    "   - Provide clear instructions about what content to focus on\n",
    "\n",
    "Research guidelines:\n",
    "- Start with a search to identify relevant sources\n",
    "- Use extract to get full content from promising URLs\n",
    "- Use crawl for documentation sites or when you need comprehensive coverage\n",
    "- Always cite your sources with URLs\n",
    "- Synthesize information from multiple sources\n",
    "- Be thorough but concise in your responses\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=system_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "response = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What are the main features of LangGraph and how does it compare to other agent frameworks?\"}]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the final response\n",
    "final_message = response[\"messages\"][-1]\n",
    "display(Markdown(final_message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View tool execution flow\n",
    "print(\"Tool Execution Flow\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for msg in response[\"messages\"]:\n",
    "    if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
    "        for tc in msg.tool_calls:\n",
    "            print(f\"Tool: {tc['name']}\")\n",
    "            args = tc.get(\"args\", {})\n",
    "            for key, value in args.items():\n",
    "                if isinstance(value, str) and len(value) > 80:\n",
    "                    value = value[:77] + \"...\"\n",
    "                print(f\"  {key}: {value}\")\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
