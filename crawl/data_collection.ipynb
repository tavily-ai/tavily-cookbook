{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and PDF Export with Tavily Crawl\n",
    "\n",
    "In this tutorial, you'll learn how to use Tavily's crawl API to collect data from websites and export the results as organized PDF files. This is perfect for creating offline archives, research documentation, or building a local knowledge base from web content.\n",
    "\n",
    "By the end of this lesson, you'll know how to:\n",
    "- Crawl websites to collect structured data\n",
    "- Process and clean the crawled content\n",
    "- Export individual pages as well-formatted PDF files\n",
    "- Organize your exported data in a structured file system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Follow these steps to set up:\n",
    "\n",
    "1. **Sign up** for Tavily at [app.tavily.com](https://app.tavily.com/home/) to get your API key.\n",
    "   \n",
    "\n",
    "2. **Copy your API keys** from your Tavily and OpenAI account dashboard.\n",
    "\n",
    "3. **Paste your API keys** into the cell below and execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To export your API keys into a .env file, run the following cell (replace with your actual keys):\n",
    "!echo \"TAVILY_API_KEY=<your-tavily-api-key>\" >> .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U tavily-python reportlab beautifulsoup4 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Your Tavily API Client\n",
    "\n",
    "The code below will instantiate the Tavily client with your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Prompt the user to securely input the API key if not already set in the environment\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY:\\n\")\n",
    "\n",
    "# Initialize the Tavily API client using the loaded or provided API key\n",
    "tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Target Website and Crawl Parameters\n",
    "\n",
    "Now let's configure our crawl to collect data from a website. We'll use Tavily's crawl API to systematically traverse the website and collect content that we can later export as PDFs.\n",
    "\n",
    "For this example, we're targeting `www.tavily.com` to collect blog posts, but you can modify the URL and instructions for any website you want to archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target website and crawl parameters\n",
    "base_url = \"www.tavily.com\"\n",
    "\n",
    "# Crawl the website with specific parameters for data collection\n",
    "crawl_result = tavily_client.crawl(\n",
    "    url=base_url,\n",
    "    limit=30,\n",
    "    max_depth=2,\n",
    "    max_breadth=30,\n",
    "    instructions=\"Blog posts and articles\",\n",
    "    format=\"text\",  # Get clean text format for better PDF conversion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully crawled 15 pages\n",
      "\n",
      "Discovered URLs:\n",
      "1. https://blog.tavily.com/\n",
      "2. https://blog.tavily.com/#/portal/\n",
      "3. https://blog.tavily.com/#main\n",
      "4. https://blog.tavily.com/ai-enablers-the-building-blocks-of-next-gen-enterprise-solutions\n",
      "5. https://blog.tavily.com/building-and-breaking-weblangchain\n",
      "... and 10 more pages\n"
     ]
    }
   ],
   "source": [
    "# Preview the crawl results\n",
    "print(f\"Successfully crawled {len(crawl_result['results'])} pages\")\n",
    "print(\"\\nDiscovered URLs:\")\n",
    "for i, page in enumerate(crawl_result[\"results\"][:5]):  # Show first 5 URLs\n",
    "    print(f\"{i+1}. {page['url']}\")\n",
    "\n",
    "if len(crawl_result[\"results\"]) > 5:\n",
    "    print(f\"... and {len(crawl_result['results']) - 5} more pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the Raw Content\n",
    "\n",
    "Let's examine a sample of the raw content from one of the crawled pages to understand the data we're working with before converting it to PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview content from the first crawled page\n",
    "if crawl_result[\"results\"]:\n",
    "    sample_page = crawl_result[\"results\"][0]\n",
    "    raw_content = sample_page.get(\"raw_content\", \"\")\n",
    "\n",
    "    print(f\"URL: {sample_page['url']}\")\n",
    "    print(f\"Content length: {len(raw_content)} characters\")\n",
    "    print(\"\\nFirst 500 characters of content:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(raw_content[:500] + \"...\" if len(raw_content) > 500 else raw_content)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up PDF Export Functions\n",
    "\n",
    "Now we'll create functions to process the crawled content and export it as well-formatted PDF files. We'll use ReportLab to create professional-looking PDFs with proper formatting, headers, and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from reportlab.lib.pagesizes import letter, A4\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.lib import colors\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_text_for_pdf(text):\n",
    "    \"\"\"Clean and prepare text content for PDF conversion\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Remove excessive whitespace and newlines\n",
    "    text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Escape special characters for ReportLab\n",
    "    text = text.replace(\"&\", \"&amp;\")\n",
    "    text = text.replace(\"<\", \"&lt;\")\n",
    "    text = text.replace(\">\", \"&gt;\")\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def generate_filename_from_url(url, max_length=50):\n",
    "    \"\"\"Generate a safe filename from a URL\"\"\"\n",
    "    # Parse the URL to get the path\n",
    "    parsed = urlparse(url)\n",
    "    path = parsed.path.strip(\"/\")\n",
    "\n",
    "    if not path:\n",
    "        path = parsed.netloc\n",
    "\n",
    "    # Clean the path for filename use\n",
    "    filename = re.sub(r\"[^\\w\\-_.]\", \"_\", path)\n",
    "    filename = re.sub(r\"_+\", \"_\", filename)\n",
    "    filename = filename.strip(\"_\")\n",
    "\n",
    "    # Truncate if too long\n",
    "    if len(filename) > max_length:\n",
    "        filename = filename[:max_length]\n",
    "\n",
    "    # Ensure it's not empty\n",
    "    if not filename:\n",
    "        filename = \"page\"\n",
    "\n",
    "    return filename\n",
    "\n",
    "\n",
    "def create_pdf_from_content(content_data, output_dir=\"exported_pdfs\"):\n",
    "    \"\"\"\n",
    "    Create a PDF file from crawled content data\n",
    "\n",
    "    Args:\n",
    "        content_data: Dict containing url, raw_content, and other metadata\n",
    "        output_dir: Directory to save the PDF files\n",
    "\n",
    "    Returns:\n",
    "        String: Path to the created PDF file\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Generate filename\n",
    "    url = content_data.get(\"url\", \"unknown\")\n",
    "    base_filename = generate_filename_from_url(url)\n",
    "    pdf_filename = f\"{base_filename}.pdf\"\n",
    "    pdf_path = os.path.join(output_dir, pdf_filename)\n",
    "\n",
    "    # Handle duplicate filenames\n",
    "    counter = 1\n",
    "    while os.path.exists(pdf_path):\n",
    "        pdf_filename = f\"{base_filename}_{counter}.pdf\"\n",
    "        pdf_path = os.path.join(output_dir, pdf_filename)\n",
    "        counter += 1\n",
    "\n",
    "    # Create PDF document\n",
    "    doc = SimpleDocTemplate(\n",
    "        pdf_path,\n",
    "        pagesize=A4,\n",
    "        topMargin=1 * inch,\n",
    "        bottomMargin=1 * inch,\n",
    "        leftMargin=0.75 * inch,\n",
    "        rightMargin=0.75 * inch,\n",
    "    )\n",
    "\n",
    "    # Get styles\n",
    "    styles = getSampleStyleSheet()\n",
    "    title_style = ParagraphStyle(\n",
    "        \"CustomTitle\",\n",
    "        parent=styles[\"Heading1\"],\n",
    "        fontSize=16,\n",
    "        spaceAfter=30,\n",
    "        textColor=colors.darkblue,\n",
    "    )\n",
    "\n",
    "    url_style = ParagraphStyle(\n",
    "        \"URLStyle\",\n",
    "        parent=styles[\"Normal\"],\n",
    "        fontSize=10,\n",
    "        textColor=colors.blue,\n",
    "        spaceAfter=20,\n",
    "    )\n",
    "\n",
    "    content_style = ParagraphStyle(\n",
    "        \"ContentStyle\", parent=styles[\"Normal\"], fontSize=11, leading=14, spaceAfter=12\n",
    "    )\n",
    "\n",
    "    # Build PDF content\n",
    "    story = []\n",
    "\n",
    "    # Add title\n",
    "    title = f\"Crawled Content from {urlparse(url).netloc}\"\n",
    "    story.append(Paragraph(title, title_style))\n",
    "\n",
    "    # Add URL\n",
    "    story.append(Paragraph(f\"Source: {url}\", url_style))\n",
    "\n",
    "    # Add crawl date\n",
    "    crawl_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    story.append(Paragraph(f\"Crawled on: {crawl_date}\", url_style))\n",
    "\n",
    "    # Add content\n",
    "    raw_content = content_data.get(\"raw_content\", \"No content available\")\n",
    "    cleaned_content = clean_text_for_pdf(raw_content)\n",
    "\n",
    "    # Split content into paragraphs\n",
    "    paragraphs = cleaned_content.split(\"\\n\\n\")\n",
    "    for para in paragraphs:\n",
    "        if para.strip():\n",
    "            story.append(Paragraph(para.strip(), content_style))\n",
    "            story.append(Spacer(1, 6))\n",
    "\n",
    "    # Build PDF\n",
    "    doc.build(story)\n",
    "\n",
    "    return pdf_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk Export All Pages to PDF\n",
    "\n",
    "Now let's export all crawled pages as organized PDF files. We'll create a structured directory and generate summary information:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_all_pages_to_pdf(crawl_results, base_output_dir=\"tavily_export\"):\n",
    "    \"\"\"\n",
    "    Export all crawled pages to organized PDF files\n",
    "\n",
    "    Args:\n",
    "        crawl_results: The results from tavily_client.crawl()\n",
    "        base_output_dir: Base directory for organizing exports\n",
    "\n",
    "    Returns:\n",
    "        dict: Summary of the export process\n",
    "    \"\"\"\n",
    "    # Create timestamped directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    export_dir = f\"{base_output_dir}_{timestamp}\"\n",
    "    pdf_dir = os.path.join(export_dir, \"pdfs\")\n",
    "\n",
    "    # Create directories\n",
    "    os.makedirs(pdf_dir, exist_ok=True)\n",
    "\n",
    "    # Export summary\n",
    "    summary = {\n",
    "        \"export_directory\": export_dir,\n",
    "        \"total_pages\": len(crawl_results[\"results\"]),\n",
    "        \"successful_exports\": 0,\n",
    "        \"failed_exports\": 0,\n",
    "        \"exported_files\": [],\n",
    "        \"errors\": [],\n",
    "    }\n",
    "\n",
    "    print(f\"📁 Creating export directory: {export_dir}\")\n",
    "    print(f\"🔄 Exporting {summary['total_pages']} pages to PDF...\")\n",
    "\n",
    "    # Process each page\n",
    "    for i, page_data in enumerate(crawl_results[\"results\"], 1):\n",
    "        try:\n",
    "            url = page_data.get(\"url\", f\"page_{i}\")\n",
    "            print(f\"  [{i}/{summary['total_pages']}] Processing: {url}\")\n",
    "\n",
    "            # Create PDF\n",
    "            pdf_path = create_pdf_from_content(page_data, pdf_dir)\n",
    "\n",
    "            summary[\"successful_exports\"] += 1\n",
    "            summary[\"exported_files\"].append(\n",
    "                {\n",
    "                    \"url\": url,\n",
    "                    \"pdf_path\": pdf_path,\n",
    "                    \"file_size\": os.path.getsize(pdf_path),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    ❌ Error processing {url}: {str(e)}\")\n",
    "            summary[\"failed_exports\"] += 1\n",
    "            summary[\"errors\"].append({\"url\": url, \"error\": str(e)})\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Run the bulk export\n",
    "export_summary = export_all_pages_to_pdf(crawl_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've successfully built a complete data collection and PDF export system using Tavily's crawl API. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
