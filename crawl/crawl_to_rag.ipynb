{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a RAG System with Tavily Crawl\n",
    "\n",
    "In this tutorial, you'll learn how to turn any website into a searchable knowledge base. We'll use Tavily's crawl API to extract information from websites, convert the content into a searchable vector index with OpenAI embeddings and in-memory Chroma vector store, and create a RAG question-answering system. This tutorial is self-contained and requires no additional setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Follow these steps to set up:\n",
    "\n",
    "1. **Sign up** for Tavily at [app.tavily.com](https://app.tavily.com/home/) to get your API key.\n",
    "\n",
    "\n",
    "2. **Sign up** for OpenAI to get your API key. Feel free to substitute any other LLM provider.\n",
    "   \n",
    "\n",
    "2. **Copy your API keys** from your Tavily and OpenAI account dashboard.\n",
    "\n",
    "3. **Paste your API keys** into the cell below and execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To export your API keys into a .env file, run the following cell (replace with your actual keys):\n",
    "!echo \"TAVILY_API_KEY=<your-tavily-api-key>\" >> .env\n",
    "!echo \"OPENAI_API_KEY=<your-openai-api-key>\" >> .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade tavily-python langchain-openai langchain-chroma langchain-community --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Your Tavily API Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will instantiate the Tavily client with your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Prompt the user to securely input the API key if not already set in the environment\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY:\\n\")\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "# Initialize the Tavily API client using the loaded or provided API key\n",
    "tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define the Target Website\n",
    "\n",
    "Now let's utilize Tavily to crawl a webpage and retrieve all of its links. Web crawling involves automatically traversing websites by following hyperlinks to uncover various web pages and URLs. Tavily's crawl feature is AI-native, offering rapid responses via parallelized, graph-based processing.\n",
    "\n",
    "For this example, we're using `www.tavily.com`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl the Tavily website\n",
    "base_url = \"www.tavily.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When crawling web pages, we can specify the output format as either \"text\" (clean text) or \"markdown\". For this tutorial, we'll use \"text\" format since it's better suited for creating embeddings later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl the base url with clean text format as output\n",
    "crawl_result = tavily_client.crawl(\n",
    "    url=base_url,\n",
    "    max_depth=3,\n",
    "    limit=100,\n",
    "    format=\"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets examine all the nested URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in crawl_result[\"results\"]:\n",
    "    print(page[\"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a second crawl with natural language `instructions` to specifically target developer documentation pages. This demonstrates how we can focus the crawler on specific types of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl the base url with clean text format as output\n",
    "focused_crawl_result = tavily_client.crawl(\n",
    "    url=base_url,\n",
    "    max_depth=3,\n",
    "    limit=100,\n",
    "    format=\"text\",\n",
    "    instructions=\"find all developer documentation pages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the results will only include developer docs from the Tavily webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in focused_crawl_result[\"results\"]:\n",
    "    print(page[\"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preview the Raw Content\n",
    "\n",
    "Let's examine a sample of the raw content from one of the crawled pages to understand the webpage data we're working with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = crawl_result[\"results\"]\n",
    "\n",
    "# Just view one sample page from the data array\n",
    "if data:\n",
    "    sample_page = data[0]  # Get the first page\n",
    "    raw_content = sample_page[\"raw_content\"]\n",
    "    print(f\"URL: {sample_page['url']}\\n\")\n",
    "    print(f\"Raw Content: \\n{raw_content}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Process Content into Documents\n",
    "\n",
    "We'll convert the crawled content into LangChain Document objects, which will allow us to:\n",
    "\n",
    "1. Maintain important metadata (source URL, page name)\n",
    "2. Prepare the text for chunking\n",
    "3. Make the content ready for vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def process_crawl_result(crawl_result):\n",
    "    # Convert the crawled content into LangChain Document objects\n",
    "    documents = []\n",
    "    # Loop through the crawl results and create a Document object for each page\n",
    "    for result in crawl_result[\"results\"]:\n",
    "        raw_content = result.get(\"raw_content\")\n",
    "        if not raw_content:  # Skip if None, empty string, or false\n",
    "            continue\n",
    "\n",
    "        doc = Document(\n",
    "            page_content=raw_content,\n",
    "            metadata={\n",
    "                \"url\": result.get(\"url\", \"\"),\n",
    "            },\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run this on the generic crawl results and the developer-specific crawl results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = process_crawl_result(crawl_result)\n",
    "documents_focussed = process_crawl_result(focussed_crawl_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Split Documents into Chunks\n",
    "\n",
    "We'll split the documents into smaller, more manageable chunks using the `RecursiveCharacterTextSplitter` and preview the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you still want to split each page into smaller chunks:\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Larger chunk size for page-level content\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "# Split the documents while preserving metadata\n",
    "all_chunks = text_splitter.split_documents(documents)\n",
    "focussed_chunks = text_splitter.split_documents(documents_focussed)\n",
    "\n",
    "# Now you have each page as a separate document with proper metadata\n",
    "print(f\"Created {len(documents)} page-level documents\")\n",
    "print(f\"Split into {len(all_chunks)} total chunks\")\n",
    "\n",
    "# Example of accessing the documents\n",
    "for i, doc in enumerate(documents[:2]):  # Print first 3 for example\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Page: {doc.metadata.get('page_name')}\")\n",
    "    print(f\"Source: {doc.metadata.get('source')}\")\n",
    "    print(f\"Content length: {len(doc.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Vector Embeddings\n",
    "\n",
    "Now we'll create vector embeddings for our document chunks using OpenAI's embedding model and store them in a Chroma vector database. This allows us to perform semantic search on our document collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Create embeddings for the documents\n",
    "embeddings = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Create a vector store from the loaded documents\n",
    "vector_store = Chroma.from_documents(documents, embeddings)\n",
    "vector_store_focussed = Chroma.from_documents(documents_focussed, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Build the Question-Answering System\n",
    "\n",
    "Finally, we'll create a retrieval-based question-answering system using gpt-4.1-mini. We use the \"stuff\" chain type, which combines all relevant retrieved documents into a single context for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", streaming=True)\n",
    "\n",
    "# Create a QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(),\n",
    ")\n",
    "qa_chain_focussed = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store_focussed.as_retriever(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test the System\n",
    "\n",
    "Let's test our RAG system by asking a question about Tavily's documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets ask a generic question about Tavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example question\n",
    "query = \"What is Tavily's production rate limit?\"\n",
    "answer = qa_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(answer[\"result\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the developer-specific index, lets ask a detailed question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example question\n",
    "query = \"tell me about the chunks_per_source parameter\"\n",
    "answer = qa_chain_focussed.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(answer[\"result\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've successfully built a complete RAG system that can:\n",
    "\n",
    "1. Crawl web content from a specific domain\n",
    "2. Process and structure the content\n",
    "3. Create vector embeddings for semantic search\n",
    "4. Answer questions based on the crawled information\n",
    "\n",
    "This approach can be extended to create knowledge bases from any website, documentation, or content repository, making it a powerful tool for building domain-specific assistants and search systems. \n",
    "\n",
    "For a more advanced implementation of this concept:\n",
    "- Try out our [hosted demo application](https://crawl-to-rag.tavily.com/)\n",
    "- View the complete [source code](https://github.com/tavily-ai/crawl2rag)\n",
    "\n",
    "\n",
    "For more information, read the crawl [API reference](https://docs.tavily.com/documentation/api-reference/endpoint/crawl) and [best practices guide](https://docs.tavily.com/documentation/best-practices/best-practices-crawl)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
