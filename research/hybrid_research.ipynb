{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0a093289",
      "metadata": {},
      "source": [
        "# Tavily Hybrid Research\n",
        "\n",
        "In this notebook, we'll walk you through the process of performing hybrid research with Tavily Research Endpoint and internal RAG.\n",
        "\n",
        "Tavily covers the web delivering you and your agents in depth, real-time research. However, you may want to combine this with your internal data, and provide it as context to our researcher to know what to focus on and to maximize coverage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bec0396",
      "metadata": {},
      "source": [
        "## The plan\n",
        "\n",
        "In this notebook specifically, we will start by generating queries for the prompt, run a vector search and then feed that into the research prompt.\n",
        "\n",
        "1. Generate queries\n",
        "2. Vector Search on internal data\n",
        "3. Generate research task for Tavily Research Endpoint\n",
        "4. Tavily Research Call with internal data as input\n",
        "5. Rewrite the research with the tavily report + RAG results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b053ea4",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q tavily-python langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dae62e2",
      "metadata": {},
      "source": [
        "Before running the next cell, **set your own LLM model and API keys** (e.g. `MODEL_OF_CHOICE`, `LLM_API_KEY`, `TAVILY_API_KEY`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e1b2dc5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "import time\n",
        "import httpx\n",
        "from IPython.display import display, Markdown\n",
        "from tavily import TavilyClient\n",
        "from dataclasses import Field\n",
        "from pydantic import BaseModel\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
        "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY:\\n\")\n",
        "\n",
        "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
        "\n",
        "# YOU NEED TO CHOOSE THE MODEL YOU WANT AND SET THE API KEY\n",
        "LLM_API_KEY = os.getenv(\"LLM_API_KEY\")\n",
        "llm = init_chat_model(\"MODEL_OF_CHOICE\")\n",
        "\n",
        "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n",
        "headers = {\"Authorization\": f\"Bearer {TAVILY_API_KEY}\"}\n",
        "\n",
        "url = \"https://api.tavily.com/research/\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdb6ae91",
      "metadata": {},
      "source": [
        "Here you need to plug in **your own internal RAG logic**.\n",
        "\n",
        "Replace the placeholder `internal_RAG_research` implementation below with code that queries your internal data (e.g. vector DB, warehouse, or docs) and returns the most relevant findings for the generated queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4c82bc1",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Queries(BaseModel):\n",
        "    queries: list[str] = Field(description=\"A list of queries to be used for the research\")\n",
        "\n",
        "# Here we just want you to implement your own internal RAG research.\n",
        "def internal_RAG_research(queries):\n",
        "    # Implement your own internal RAG research\n",
        "    return \"Internal RAG results\"\n",
        "\n",
        "structured_llm = llm.with_structured_output(Queries)\n",
        "task_description = \"I launched a new AI Meeting Notes feature â€” now I want to evaluate customer feedback and what competitor products are out there.\"\n",
        "# Generate subqueries to get full coverage of internal data\n",
        "queries = structured_llm.invoke(\"Here is my research task: {task_description}. Break down the task into a list of 5 queries to be used for RAG on my internal data. I have internal data about usage, feedback, and initial R&D on the feature.\" )\n",
        "internal_rag_results = internal_RAG_research(queries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf8e7538",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a web research task\n",
        "web_research_task = llm.invoke(\n",
        "    f\"\"\"\n",
        "Using the user's research task and the internal RAG findings below, generate ONE concise research task for a web-research agent.\n",
        "\n",
        "User Research Task:\n",
        "{task_description}\n",
        "\n",
        "Internal Findings:\n",
        "{internal_rag_results}\n",
        "\n",
        "Your goal:\n",
        "Identify what information is still missing or requires external validation, and describe exactly what the web-research agent should investigate.\n",
        "\n",
        "The research task should:\n",
        "- Focus only on what needs external research.\n",
        "- Specify key topics, entities, or sources to look into (e.g., reviews, news, competitors, documentation, benchmarks, financials, regulations, etc.).\n",
        "- Not restate internal findings.\n",
        "- Be a single clear paragraph or bullet list that an autonomous agent can execute.\n",
        "\n",
        "Output ONLY the final research task.\n",
        "    \"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cded0974",
      "metadata": {},
      "outputs": [],
      "source": [
        "result = tavily_client.research(input=web_research_task, model=\"pro\")\n",
        "request_id = result[\"id\"]\n",
        "\n",
        "research_report = \"\"\n",
        "sources = []\n",
        "\n",
        "# Poll every 10 seconds\n",
        "while True:\n",
        "    response = httpx.get(url + request_id, headers=headers)\n",
        "    response_json = response.json()\n",
        "\n",
        "    status = response_json[\"status\"]\n",
        "    if status == \"completed\":\n",
        "        research_report = response_json[\"content\"]\n",
        "        sources = response_json[\"sources\"]\n",
        "        break\n",
        "\n",
        "    if status == \"failed\":\n",
        "        raise RuntimeError(f\"Research failed: {response_json['error']}\")\n",
        "\n",
        "    print(f\"Status: {status}... polling again in 10 seconds\")\n",
        "    time.sleep(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85d130d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_synthesis_prompt = f\"\"\"You research task was: {task_description}.\n",
        "\n",
        "Here is the internal RAG results:\n",
        "{internal_rag_results}\n",
        "\n",
        "Here is the Tavily research report:\n",
        "{research_report}\n",
        "\n",
        "Take these results and generate a final report to accomplish the research task.\n",
        "\n",
        "Make sure you properly in text cite sources as you did with the web results, but if you use internal data, make sure to cite that as well.\n",
        "\n",
        "Output the final report as a markdown document.\n",
        "\"\"\"\n",
        "\n",
        "final_report = llm.invoke(final_synthesis_prompt)\n",
        "print(\"Research Complete!\")\n",
        "report = Markdown(final_report)\n",
        "display(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b032866",
      "metadata": {},
      "outputs": [],
      "source": [
        "sources"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
