{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "03d30c67",
      "metadata": {},
      "source": [
        "## Prompt Clarification for Research\n",
        "\n",
        "Deep research models work best with detailed, well-structured prompts. When users provide vague or underspecified queries, we can guide them toward providing more detail by gathering clarifying information before invoking the research service.\n",
        "\n",
        "This notebook demonstrates how to use an LLM to interactively refine a research query through clarifying questions, ensuring the final prompt is specific, unambiguous, and aligned with what the user actually wants to learn.\n",
        "\n",
        "\n",
        "**How it works**\n",
        "1. **Initial input**: User provides a research query (potentially vague or incomplete)\n",
        "2. **Clarification loop**: An LLM evaluates the query and asks targeted follow-up questions to gather missing details\n",
        "3. **Iterative refinement**: The user responds, and the loop continues until the LLM determines it has enough context (or a max iteration limit is reached)\n",
        "4. **Research**: The refined, detailed prompt is passed to Tavily's research API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44331cdb",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q tavily-python pydantic\n",
        "%pip install -U \"langchain[openai]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b0f9c65",
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "from tavily import TavilyClient\n",
        "\n",
        "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
        "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY:\\n\")\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY:\\n\")\n",
        "\n",
        "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
        "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n",
        "\n",
        "headers = {\"Authorization\": f\"Bearer {TAVILY_API_KEY}\"}\n",
        "url = \"https://api.tavily.com/research/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a2b19e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from pydantic import BaseModel, Field\n",
        "import time\n",
        "import httpx\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "model = init_chat_model(\"gpt-5.1-mini\", model_provider=\"openai\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b2c3d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClarificationResponse(BaseModel):\n",
        "    \"\"\"Structured response for query clarification.\"\"\"\n",
        "    needs_clarification: bool = Field(description=\"True if more info needed, False if ready to research\")\n",
        "    message: str = Field(description=\"Either follow-up questions OR the refined research query\")\n",
        "\n",
        "PROMPT = \"\"\"You are a research assistant refining a research query through conversation.\n",
        "\n",
        "Original topic: {query}\n",
        "\n",
        "Conversation:\n",
        "{conversation}\n",
        "\n",
        "If you need more details, set needs_clarification=True and ask 2-3 questions about:\n",
        "- Specific subtopics, time frame, depth needed, relevant contexts, or source types\n",
        "\n",
        "If you have enough context, set needs_clarification=False and provide a detailed refined query.\n",
        "\"\"\"\n",
        "\n",
        "def clarify(query: str, conversation: list) -> ClarificationResponse:\n",
        "    conv_text = \"\\n\".join(f\"{m['role'].title()}: {m['content']}\" for m in conversation) or \"(none)\"\n",
        "    return model.with_structured_output(ClarificationResponse).invoke(\n",
        "        PROMPT.format(query=query, conversation=conv_text)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5f6g7h8",
      "metadata": {},
      "source": [
        "## Interactive Query Refinement\n",
        "\n",
        "> Note: This example uses `input()` for interactive prompts. If you're running in an environment that doesn't support stdin for notebooks (for example, some IDEs or hosted runners), you can replace the `input()` calls with hard-coded strings for `initial_query` and the follow-up replies.\n",
        "\n",
        "> This cell is primarily meant as a simple example of how to implement an interactive clarification loopâ€”feel free to adapt the flow and UX for your own application.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93146532",
      "metadata": {},
      "outputs": [],
      "source": [
        "max_iterations = 3\n",
        "\n",
        "# Get initial query from user\n",
        "initial_query = input(\"What would you like to research?\\n> \")\n",
        "conversation = []\n",
        "\n",
        "# Refinement loop\n",
        "for i in range(max_iterations):\n",
        "    response = clarify(initial_query, conversation)\n",
        "    \n",
        "    if not response.needs_clarification:\n",
        "        refined_query = response.message\n",
        "        print(f\"\\nâœ… Refined query:\\n{refined_query}\")\n",
        "        break\n",
        "    \n",
        "    print(f\"\\nðŸ¤– Assistant:\\n{response.message}\")\n",
        "    conversation.append({\"role\": \"assistant\", \"content\": response.message})\n",
        "    \n",
        "    user_input = input(\"\\n> \")\n",
        "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
        "else:\n",
        "    # Max iterations reached - force final query\n",
        "    response = clarify(initial_query, conversation)\n",
        "    refined_query = response.message\n",
        "    print(f\"\\nâœ… Refined query:\\n{refined_query}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g7h8i9j0",
      "metadata": {},
      "source": [
        "## Execute Research\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "075f2b15",
      "metadata": {},
      "outputs": [],
      "source": [
        "result = tavily_client.research(input=refined_query, model=\"mini\")\n",
        "request_id = result[\"id\"]\n",
        "\n",
        "# Poll until complete\n",
        "while True:\n",
        "    resp = httpx.get(f\"{url}{request_id}\", headers=headers).json()\n",
        "    if resp[\"status\"] == \"completed\":\n",
        "        break\n",
        "    if resp[\"status\"] == \"failed\":\n",
        "        raise RuntimeError(f\"Research failed: {resp['error']}\")\n",
        "    print(f\"Status: {resp['status']}... polling in 10s\")\n",
        "    time.sleep(10)\n",
        "\n",
        "print(\"\\nâœ… Research Complete!\\n\")\n",
        "display(Markdown(resp[\"content\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a070abc",
      "metadata": {},
      "outputs": [],
      "source": [
        "resp.get(\"sources\", [])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
